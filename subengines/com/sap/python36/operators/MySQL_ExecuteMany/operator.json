{
    "description": "MySQL Replication",
    "component": "com.sap.system.python3Operator",
    "versionStatus": "active",
    "inports": [
        {
            "name": "File",
            "type": "message"
        }
    ],
    "outports": [
        {
            "name": "out",
            "type": "string"
        },
        {
            "name": "stop",
            "type": "string"
        }
    ],
    "icon": "puzzle-piece",
    "iconsrc": "aws-rds.svg",
    "config": {
        "$type": "http://sap.com/vflow/MySQL_ExecuteMany.configSchema.json",
        "script": "import pandas as pd\r\nimport io\r\nimport mysql.connector\r\nfrom time import perf_counter\r\nimport sys\r\nfrom pympler import asizeof\r\nimport json\r\n\r\ndef get_tab_cols_keys(abtyplist):\r\n    # query = f'SHOW columns FROM {table}'\r\n    # cursor.execute(query)\r\n    # lista = cursor.fetchall()\r\n    keys = []\r\n    cols = []\r\n    # for row in lista:\r\n    #     cols.append(row[0])\r\n    #     if row[3] == 'PRI':\r\n    #         keys.append(row[0])\r\n    for i in abtyplist:\r\n        cols.append(i.get('Field').get('COLUMNNAME'))\r\n        if i.get('Field').get('KEY') == 'X':\r\n            keys.append(i.get('Field').get('COLUMNNAME'))\r\n    cols.append(\"Table\")\r\n    cols.append(\"Flag\")\r\n    return cols,keys\r\n\r\ndef get_db_cursor(connProp):\r\n    #Connect to DB\r\n    mydb = mysql.connector.connect(\r\n          host= connProp['host'],\r\n          port=connProp['port'],  \r\n          user=connProp['user'],\r\n          database=connProp['database'],\r\n          password=connProp['password']\r\n        )\r\n    #Get DB Cursor  \r\n    return mydb,mydb.cursor()\r\n    \r\ndef build_insert(column_count,df,tableName):\r\n    api.logger.info(\"entered insert\")\r\n    #Placeholder string\r\n    nstr = ''\r\n    for l in range(0,column_count):\r\n        if l == column_count - 1:\r\n            nstr += '%s'\r\n        else:    \r\n            nstr += '%s,'\r\n    api.logger.info(\"loopend\")        \r\n    #generating data in necessary format(list of tuples)\r\n    final_data_ins = []\r\n\r\n    #df.fillna(\"\", inplace = True)\r\n    #api.logger.info(f\"before none - {df['XNETB']}\")\r\n    #df.to_numpy()[df==''] = None\r\n    #df.values[df == ''] = None\r\n    #api.logger.info(f\"after none - {df['XNETB']}\")\r\n    \r\n    api.logger.info(\"strmap starts\")\r\n\r\n    df = df.applymap(str)\r\n\r\n    api.logger.info(\"strmap mid\")\r\n    \r\n    #api.logger.info(df.head())\r\n    #final_data_ins = df.to_records(index=False).tolist()    \r\n    #api.logger.info(df.to_records(index=False))\r\n    start_time_none = perf_counter()\r\n    tmp_list = df.to_numpy()\r\n    tmp_list[tmp_list==''] = None\r\n    final_data_ins = list(map(tuple,tmp_list))\r\n    api.logger.info(f\"time for none{perf_counter() - start_time_none}\")\r\n    api.logger.info(\"strmap ends\")\r\n\r\n    #concatenate all the records into insert query\r\n    insert_query = f\"insert into {tableName} values ({nstr})\"\r\n\r\n    api.logger.info(\"left insert\")\r\n    return insert_query, final_data_ins\r\n\r\ndef build_update(cols,keys,df,tableName):\r\n    \r\n    #generating data in necessary format(list of tuples)\r\n    final_data_upd = []\r\n    \r\n    #pd.set_option('display.max_columns', None)\r\n    #df.fillna(\"\", inplace = True)\r\n    #api.logger.info(f\"before none - {df}\")\r\n    #df.to_numpy()[df==''] = None\r\n    #df.values[df == ''] = None\r\n    #api.logger.info(f\"after none - {df['XNETB']}\")\r\n    \r\n    df = df.applymap(str)\r\n    #final_data_upd = df.to_records(index=False).tolist()\r\n    start_time_none = perf_counter()\r\n    tmp_list = df.to_numpy()\r\n    tmp_list[tmp_list==''] = None\r\n    final_data_upd = list(map(tuple,tmp_list))\r\n    api.logger.info(perf_counter() - start_time_none)\r\n    \r\n    #get list of cols w/o keys\r\n    s = set(keys)\r\n    upd_cols = [x for x in cols if x not in s]\r\n    \r\n    #Build update Query\r\n    col_str = ', '.join('`{0}`'.format(k) for k in cols)\r\n    duplicates = ', '.join('{0}=VALUES({0})'.format(k) for k in upd_cols)\r\n    place_holders = ', '.join('%s'.format(k) for k in cols)\r\n    \r\n    query = \"INSERT INTO {0} ({1}) VALUES ({2})\".format(tableName, col_str, place_holders)\r\n    query = \"{0} ON DUPLICATE KEY UPDATE {1}\".format(query, duplicates)\r\n    \r\n    return query,final_data_upd\r\n    \r\ndef build_delete(cols,keys,df,tableName):\r\n    \r\n    #generating data in necessary format(list of tuples)\r\n    #final_data_del = []\r\n\r\n    df1 = df[keys]\r\n    final_data_del = list(map(tuple,df1.to_numpy()))\r\n    #get list of cols w/o keys\r\n\r\n    #Build delete Query\r\n    col = ', '.join(k for k in keys)\r\n    place_holders = ', '.join(str(k) for k in final_data_del)\r\n    \r\n    query = \"DELETE FROM {0} WHERE ({1}) IN ({2})\".format(tableName, col, place_holders)\r\n    return query\r\n    \r\ndef execute(mydb,cursor,stmt,final_data):\r\n    api.logger.info(\"entered execute\")\r\n    #api.logger.info(final_data)\r\n    start = perf_counter()\r\n    #Commit to DB\r\n    api.logger.info(stmt)\r\n    api.logger.info(str(sys.getsizeof(final_data)))\r\n    api.logger.info(str(asizeof.asizeof(final_data)))\r\n    try:\r\n        cursor.executemany(stmt, final_data)\r\n        mydb.commit()\r\n        result = f\"Success - {cursor.rowcount} inserted\"\r\n    except mysql.connector.IntegrityError as e:\r\n        api.logger.info(\"exception\")\r\n        result = 'Failure'\r\n        api.logger.info(e)\r\n        api.logger.info(perf_counter() - start)\r\n        api.propagate_exception(e)\r\n    # except mysql.connector.errors.DatabaseError as d:    \r\n    #     api.logger.info(\"exception\")\r\n    #     result = 'Failure'\r\n    #     api.logger.info(d)\r\n    #     api.logger.info(perf_counter() - start)\r\n    #     api.propagate_exception(d)\r\n        \r\n    api.logger.info(result)\r\n    api.logger.info(\"Time Taken for execute\")\r\n    api.logger.info(perf_counter() - start)\r\n    \r\n    return result\r\n    \r\ndef on_input(File):\r\n    \r\n    start_time = perf_counter()\r\n    \r\n    #Read attributes\r\n    var = json.dumps(File.attributes) \r\n    attr = json.loads(var)\r\n    \r\n    #CHECK If its last batch\r\n    if \"message.lastBatch\" in attr:\r\n        api.send(\"stop\",\"stop\")    \r\n    else:\r\n        dtypelist = attr.get('ABAP').get('Fields')\r\n        abaptypelist = attr.get('metadata')\r\n        abtyplist = [i for i in abaptypelist if not (i.get('Field').get('ABAPTYPE') == '')]\r\n        \r\n        #Read Operator Config\r\n        connProp = api.config.Connection['connectionProperties']\r\n        tableName = str(api.config.Table)\r\n        \r\n        #Get Connection and Cursor\r\n        mydb,cursor = get_db_cursor(connProp)\r\n        \r\n        #Get Cols and Keys from MySQL\r\n        cols,keys = get_tab_cols_keys(abtyplist)\r\n        \r\n        #Read data to DataFrame\r\n        data_stream = io.StringIO(File.body)\r\n        df = pd.read_csv(data_stream,names=cols,index_col=False,na_filter=False)\r\n        \r\n        #Split the Upd , Delete and Ins Data\r\n        df_upd = df[df[\"Flag\"] =='U']\r\n        #df_ins = df[(df[\"Flag\"] == 'I') | (df[\"Flag\"].isnull())]\r\n        df_ins = df[(df[\"Flag\"] == 'I') | (df[\"Flag\"] == '')]\r\n        df_del = df[df[\"Flag\"] == 'D']\r\n        \r\n        #Remove the last 2 columns(coming from SLT)\r\n        df = df.iloc[: , :-2]\r\n        df_upd = df_upd.iloc[: , :-2]\r\n        df_ins = df_ins.iloc[: , :-2]\r\n        df_del = df_del.iloc[: , :-2]\r\n        \r\n        #getting the column count for each file given\r\n        column_count = len(df.columns)\r\n        result = ''\r\n        \r\n        api.logger.info(\"Count Start\")\r\n        api.logger.info(df_ins.shape)\r\n        api.logger.info(df_upd.shape)\r\n        api.logger.info(\"Count End\")\r\n        \r\n        api.logger.info(\"time for processing\")\r\n        api.logger.info(perf_counter() - start_time)\r\n        \r\n        if df_ins.shape[0] \u003e 0:\r\n            #Get Insert query and data\r\n            insert_query,insert_data = build_insert(column_count,df_ins,tableName)\r\n            #Execute insert\r\n            result_insert = execute(mydb,cursor,insert_query,insert_data)\r\n        \r\n        if df_upd.shape[0] \u003e 0:\r\n            #Get Update query and data\r\n            update_query,update_data = build_update(cols[:-2],keys,df_upd,tableName)\r\n            #Execute update\r\n            result_update = execute(mydb,cursor,update_query,update_data)    \r\n \r\n        if df_del.shape[0] \u003e 0:\r\n            #Get Delete query \r\n            delete_query = build_delete(cols[:-2],keys,df_del,tableName)\r\n            #Execute Delete\r\n            #result_delete = execute(mydb,cursor,delete_query,delete_data)  \r\n            try:\r\n                cursor.execute(delete_query)\r\n                mydb.commit()\r\n                result = f\"Success - {cursor.rowcount} inserted\"\r\n            except mysql.connector.IntegrityError as e:\r\n                api.logger.info(\"exception\")\r\n                result = 'Failure'\r\n                api.logger.info(e)\r\n                api.logger.info(perf_counter() - start)\r\n                api.propagate_exception(e)\r\n        #close the connection\r\n        cursor.close()\r\n        mydb.close()       \r\n    \r\n        api.send(\"out\", result)\r\n\r\napi.set_port_callback(\"File\", on_input)",
        "user": "admin"
    },
    "tags": {
        "mysqlaurora": "8.0.12",
        "python36": "",
        "tornado": "5.0.2"
    }
}